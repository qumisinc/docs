---
title: "Qumis Prompt Library"
description: "Reusable prompts for Qumis documentation and operations"
noindex: true
---

The Qumis Prompt Library is a collection of reusable prompts for various documentation, engineering, and operational tasks. This library helps maintain consistency and efficiency across the team.

## Documentation Prompts

<Info>
  Prompts for creating, reviewing, and transforming documentation.
</Info>

### Convert to Markdown

Convert content to clean Markdown format with proper formatting and escaping.

<Accordion title="Convert to Markdown Prompt">
```markdown
Convert this to Markdown. Try to capture the original format as accurately as possible.

Return this as Markdown in a code block (i.e., ```Markdown ...```). Escape any characters necessary. Make sure that any Markdown code block is escaped properly. Do not respond with anything but the code block.
```
</Accordion>

### Summarization

Create concise, audience-specific summaries of reports and documents.

<Accordion title="Summarization Prompt">
```markdown
Create a ~200 word summarization of the most important data from this report for <AUDIENCE>. Make it concise, extremely clear, and exceptionally readable.
```
</Accordion>

### Create llms-full.txt

Generate comprehensive documentation files from codebases or documentation sites, optimized for LLM consumption.

<Accordion title="llms-full.txt for Documentation Sites">
```markdown
You will be creating a Markdown file named llms-full.txt that contains all essential information from an input file, converted to clean Markdown.

Your task is to process this input and create a single Markdown file that follows these rules:

1. Begin with "# <Project Name>" on its own line.
2. Immediately follow with a concise 1-paragraph intro (no block-quote needed).
3. Inline the full Markdown of every important page in logical order (e.g. Quick Start, Concepts, API Reference, Guides).
4. Preserve headings (#–####), code fences, lists, tables, and images (![alt](url)). Strip any extraneous HTML, scripts, or navigation.
5. Do not add external links — inline the content instead.
6. Target ≤ 100 KB of output; summarise verbose or repetitive sections if needed, but keep code samples intact.

Follow these steps to create the output:
1. Identify the project name and create the top-level heading.
2. Write a concise 1-paragraph introduction summarizing the project.
3. Determine the logical order of important pages (e.g., Quick Start, Concepts, API Reference, Guides).
4. For each important page:
    a. Convert the content to clean Markdown.
    b. Preserve headings, code fences, lists, tables, and images.
    c. Remove any extraneous HTML, scripts, or navigation elements.
    d. Inline any external links by including the relevant content directly.
5. If the total content exceeds 100 KB, summarize verbose or repetitive sections while keeping code samples intact.

Provide your output as Markdown. The content of these tags should be exactly what would be written to the /llms-full.txt file, with no additional commentary or explanation.
```
</Accordion>

<Accordion title="llms-full.txt for Code Repositories (Strategic Extraction)">
```markdown
You will be creating a Markdown file named llms-full.txt that contains all essential information from a code repository, converted to clean Markdown format optimized for LLM consumption and understanding, with strategic content extraction rather than complete file dumps.

Your task is to process the repository input and create a single comprehensive Markdown file that follows these rules:

1. Begin with "# <Project Name>" on its own line.
2. Immediately follow with a concise 1-paragraph intro describing what the project does, its main purpose, and primary programming language(s) used.
3. Include a **System Architecture Overview** explaining the application pattern, key components, and data flow.
4. Add a **Module Dependencies Map** showing critical file relationships and import chains.
5. Include a comprehensive file structure overview with file type indicators and role descriptions.
6. Extract and present **key code segments** from important files in logical order following the priority hierarchy below.
7. For each file, use appropriate code fences with language specification and semantic markers.
8. Focus on **interfaces, signatures, core logic, and architectural patterns** rather than complete implementations.
9. Apply smart filtering and extraction strategies for optimal LLM comprehension.
10. Target ≤ 100 KB with dynamic adjustment, prioritizing understanding over completeness.

**Enhanced File Priority Hierarchy (highest to lowest):**
1. **Core Entry Points**: main.py, index.js, app.py, server.js, main.go, etc.
2. **Primary Business Logic**: Core models, controllers, services, domain logic
3. **Configuration Files**: package.json, requirements.txt, Cargo.toml, go.mod, tsconfig.json, webpack.config.js
4. **Key Integration Points**: Database models, API interfaces, external service connectors
5. **Infrastructure**: Dockerfile, docker-compose.yml, .github/workflows, deployment configs
6. **Type Definitions**: .d.ts files, interface definitions, schema files
7. **Critical Utilities**: Helper functions, shared libraries, middleware
8. **Representative Tests**: Key unit tests, integration tests that demonstrate usage patterns
9. **Documentation**: README.md, API documentation, architectural decisions

[Full prompt continues with architecture overview, dependency mapping, extraction strategies, and quality assurance sections...]
```
</Accordion>

## Engineering Prompts

<Info>
  Prompts for code generation, review, debugging, and technical tasks.
</Info>

### Self-Review

Perform comprehensive self-reviews to improve code quality and catch issues early. This two-step process helps LLMs evaluate and improve their own outputs.

<Accordion title="Self-Review Process">
#### Step 1: Perform Self-Review

**v0.1 - Basic Self-Review**
```markdown
Perform a comprehensive self-review of your previous output. This process involves analyzing your response, evaluating relevant attributes, and providing constructive feedback for improvement. Your goal is to produce a thorough, objective, and actionable self-review. Add scores (0.0-10.0) for each of your evaluations then a final overall score.
```

**v0.2 - Enhanced Self-Review Options**

**Review Last Message:**
```markdown
Perform a comprehensive self-review of your LAST message and my message before that. First, describe the objective of what I had asked for and what you had replied with in your response. This process involves analyzing your response, evaluating relevant attributes, and providing constructive feedback for improvement. Your goal is to produce a thorough, objective, and actionable self-review. Add scores (0.0-10.0) for each of your evaluations then a final overall score.
```

**Review Modifications (General):**
```markdown
Perform a comprehensive self-review. First, describe the objective of what I had asked for and what you had replied with in your response(s). This process involves analyzing your response(s), evaluating relevant attributes, and providing constructive feedback for improvement. Your goal is to produce a thorough, objective, and actionable self-review. Add scores (0.0-10.0) for each of your evaluations then a final overall score.
```

#### Step 2: Revise/Improve Output

**v0.1 - Basic Revision**
```markdown
Analyze your self-review and suggestions. Revise the original work to address any concerns and/or improvements with the goal of increasing the scores. Maintain its original purpose and style.
```

**v0.2 - Enhanced Revision**
```markdown
Review your previous analysis and apply the suggested improvements to the process described in the original content. Focus on enhancing the score for the attributes that you have scored, while maintaining the original intent and structure.
```

<Note>
  You can repeat the self-review process multiple times for iterative improvement. As noted by @Erik Nomitch: "Since LLMs are more performant at review/analysis than they are at one-shot generation, I find that doing this self-review process can drastically improve many types of outputs."
</Note>
</Accordion>

### Create Retool SQL Queries

Generate SQL queries specifically formatted for Qumis Retool applications with proper naming conventions and parameter syntax.

<Accordion title="Retool SQL Query Generator">

```markdown
Create a Retool query and name for: <DESCRIPTION>

Review the database schema defined in `schema.json` and use it for context.

You will return only the following:
• A query name following this naming convention: `q_[description][timeframe/type][grouping]`
• A PostgreSQL SQL statement, using Retool's `{{ }}` syntax for parameters
• Use only tables, columns, and enums that exist in `schema.json`

Examples of valid query names:
• `q_chats_weekly_count_by_org`
• `q_chats_weekly_count_by_user`
• `q_chats_6week_count_by_org`
• `q_chats_6week_count_by_user`
• `q_chat_data_validation`

**Format:**

Return your answer in a single code block like this:

```sql
-- name: q_example_query_type_group
SELECT * FROM example_table WHERE id = {{ id }};
```

Do not include any explanatory text—only the code block.
```

**Instructions:**
1. Add the `schema.json` file (available from the Qumis engineering team)
2. Replace `<DESCRIPTION>` with your query requirement
3. Run the prompt to generate the SQL query and name
4. In Retool, go to "Edit App" → "Code"
5. Duplicate an existing query
6. Replace the SQL with your generated query and rename it
7. Click Run to test

**Example Descriptions:**
- "The total number of chats, per organization, within the last 6 weeks"
- "The average number of users per organization"
</Accordion>

### Backfilling Linear Issues

Convert code changes and commits into properly formatted Linear tickets for documentation.

<Accordion title="Backfilling Linear Issues">

```markdown
# OBJECTIVE

I'd like to backfill our Linear issues with new tickets that are already done and in the codebase but don't exist in Linear yet.

Your objective is to review the code and data in the <CHANGES> tags and create Linear tickets (in Markdown format) from them.

Here are some examples of Linear tickets in Markdown:

```markdown
# Only allows users to upload appropriate document types

Currently when a user goes to upload a document, the UI states only PDF format only however users are able to upload other types of documents such as .pngs that do not process correctly.

Users should only be allowed to upload .pdf
```

```markdown
# Add persistent environment information badges

Implement status badges for all non-customer facing environments. This is to limit mistakes and increase flow speed.

Since we will all be switching deployment environments pretty frequently, we should have an indicator that we on a certain environment (unless it is production) in the header.

I don't want one of us to accidentally do something to our production environment when we think we are on dev, qa, uat, etc.

It should be persistent across all pages and and fixed with the top z-index.
```

<CHANGES>
[Paste your code changes or commit history here]
</CHANGES>
```
</Accordion>

## Operations Prompts

<Info>
  Prompts for infrastructure management, monitoring, and operational workflows.
</Info>

### Alignment on Objective

Identify gaps in understanding between user intent and AI interpretation to improve task alignment.

<Accordion title="Alignment on Objective">
```markdown
Review our interaction(s) so far. Think about the differences between:
- My understanding of what I've asked/prompted you for (should usually be a high level since it's in my mental model)
- Your understanding of my mental model transferred to you via chats and prompts.

This is an inherently lossy process. Identify potential gaps in our understanding of the objective. How can we reach parity?

First identify the potential differences in our understandings and anything that is ambiguous or unclear. Then come up with a list of questions for me which, if answered, will increase our likelihood of being aligned on what my true objective is. For each of the questions, add a score (0.0-1.0) of how much you think the answered question will improve our alignment on objective.
```
</Accordion>

### Chat Dump

Create a comprehensive backup of conversation data for knowledge transfer or session restoration.

<Accordion title="Chat Dump">
```markdown
Create a comprehensive JSON dump of all of the data so far in this conversation so that if given to you (or another LLM) in a new instance, you could "ingest" it and retain the knowledge from this entire conversation.
```
</Accordion>

## Customer Support Prompts

<Info>
  Prompts for customer communication, issue resolution, and support workflows.
</Info>

*Customer support prompts will be added as they are developed and tested with the Qumis support team.*

## Meta-Prompting Techniques

<Info>
  Advanced prompt engineering techniques for creating and optimizing prompts.
</Info>

### Generate/Evaluate Method

A systematic approach to creating optimized prompts using the LLM's own capabilities to generate and evaluate prompts.

<Accordion title="Generate/Evaluate Method">
This method leverages the LLM's weights to influence prompt generation and evaluation, often producing better results than manually crafted prompts.

**Process:**

1. **Generate a prompt engineering guide:**
   ```markdown
   Generate a detailed prompt engineering guide. The audience is <role>.
   ```
   (Example roles: "book authors", "software developers", "customer support reps")

2. **Provide few-shot examples:**
   Paste 5 examples of input/output pairs showing how you want the prompt to work

3. **Generate the prompt:**
   ```markdown
   Generate a prompt that could have generated the examples' outputs, and include a better set of examples.
   ```

4. **In a new chat, generate evaluation guide:**
   ```markdown
   Generate a detailed prompt evaluation guide. The audience is <role>.
   ```

5. **Evaluate the prompt:**
   Paste the generated prompt and instruct:
   ```markdown
   Evaluate the prompt.
   ```

6. **Generate alternatives:**
   ```markdown
   Generate 3 improved alternative prompts.
   ```

7. **Select and refine:**
   Choose the best alternative and edit as necessary

<Tip>
  Use a model from the same family as your target deployment for best results.
</Tip>
</Accordion>

### Lyra - AI Prompt Optimizer

A master-level prompt optimization specialist that transforms any user input into precision-crafted prompts.

<Accordion title="Lyra Prompt Optimizer">
```markdown
You are Lyra, a master-level AI prompt optimization specialist. Your mission: transform any user input into precision-crafted prompts that unlock AI's full potential across all platforms.

## THE 4-D METHODOLOGY

### 1. DECONSTRUCT
- Extract core intent, key entities, and context
- Identify output requirements and constraints
- Map what's provided vs. what's missing

### 2. DIAGNOSE
- Audit for clarity gaps and ambiguity
- Check specificity and completeness
- Assess structure and complexity needs

### 3. DEVELOP
- Select optimal techniques based on request type:
  - **Creative** → Multi-perspective + tone emphasis
  - **Technical** → Constraint-based + precision focus
  - **Educational** → Few-shot examples + clear structure
  - **Complex** → Chain-of-thought + systematic frameworks
- Assign appropriate AI role/expertise
- Enhance context and implement logical structure

### 4. DELIVER
- Construct optimized prompt
- Format based on complexity
- Provide implementation guidance

## OPTIMIZATION TECHNIQUES

**Foundation:** Role assignment, context layering, output specs, task decomposition

**Advanced:** Chain-of-thought, few-shot learning, multi-perspective analysis, constraint optimization

**Platform Notes:**
- **ChatGPT/GPT-4:** Structured sections, conversation starters
- **Claude:** Longer context, reasoning frameworks
- **Gemini:** Creative tasks, comparative analysis
- **Others:** Apply universal best practices

## OPERATING MODES

**DETAIL MODE:**
- Gather context with smart defaults
- Ask 2-3 targeted clarifying questions
- Provide comprehensive optimization

**BASIC MODE:**
- Quick fix primary issues
- Apply core techniques only
- Deliver ready-to-use prompt

## RESPONSE FORMATS

**Simple Requests:**
\`\`\`
**Your Optimized Prompt:**
[Improved prompt]

**What Changed:** [Key improvements]
\`\`\`

**Complex Requests:**
\`\`\`
**Your Optimized Prompt:**
[Improved prompt]

**Key Improvements:**
• [Primary changes and benefits]

**Techniques Applied:** [Brief mention]

**Pro Tip:** [Usage guidance]
\`\`\`

## WELCOME MESSAGE (REQUIRED)

When activated, display EXACTLY:

"Hello! I'm Lyra, your AI prompt optimizer. I transform vague requests into precise, effective prompts that deliver better results.

**What I need to know:**
- **Target AI:** ChatGPT, Claude, Gemini, or Other
- **Prompt Style:** DETAIL (I'll ask clarifying questions first) or BASIC (quick optimization)

**Examples:**
- "DETAIL using ChatGPT — Write me a marketing email"
- "BASIC using Claude — Help with my resume"

Just share your rough prompt and I'll handle the optimization!"

## PROCESSING FLOW

1. Auto-detect complexity:
   - Simple tasks → BASIC mode
   - Complex/professional → DETAIL mode
2. Inform user with override option
3. Execute chosen mode protocol
4. Deliver optimized prompt

**Memory Note:** Do not save any information from optimization sessions to memory.
```

<Note>
  Source: [Reddit r/ChatGPT](https://www.reddit.com/r/ChatGPT/comments/1lnfcnt/after_147_failed_chatgpt_prompts_i_had_a/)
</Note>
</Accordion>

## Contributing to the Library

To add a new prompt to this library:

1. Identify the appropriate category for your prompt
2. Add a descriptive title and purpose
3. Include the prompt template with clear placeholders
4. Provide examples of how to use the prompt
5. Document any specific requirements or context needed

<Note>
  When adding prompts, consider including:
  - Clear variable placeholders using [brackets] or `<VARIABLES>`
  - Example inputs and expected outputs
  - Any prerequisites or context required (like schema.json for SQL queries)
  - Tips for customizing the prompt
  - Version numbers for iterative improvements
</Note>

## Best Practices

- Keep prompts concise and focused on a single task
- Use consistent formatting and placeholder conventions
- Test prompts before adding them to the library
- Update existing prompts based on team feedback
- Document any tool-specific requirements
- Include attribution when prompts are sourced from external resources

## Resources

- [Shumerprompt.com](https://shumerprompt.com/) - Prompt engineering resources
- [Reddit r/ChatGPT](https://www.reddit.com/r/ChatGPT/) - Community-driven prompt discoveries
- [Qumis llms.txt](https://www.notion.so/Qumis-llms-txt-2165d92ed08b80c589d4d3ad34f665d6) - Qumis-specific context for prompts

---

*This prompt library is maintained by the Qumis team. For questions or suggestions, reach out in #engineering on Slack.*